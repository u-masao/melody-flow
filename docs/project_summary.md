# 【動画制作用】Melody Flow V3 プロジェクトサマリー（最終詳細版）

## 1. プロジェクト概要

**プロジェクト名:** Melody Flow (メロディ・フロー)
**キャッチコピー案:** Your Rhythm, AI's Melody. （あなたのリズムが、AIのメロディになる。）
**概要:**
Melody Flowは、人間とAIがリアルタイムでセッションを行う、新時代の音楽共創ツールです。ユーザーは「リズム」と「演奏ニュアンス（強弱・音の長さ）」だけを入力し、AIがその瞬間のコード進行に最適な「メロディ（音の高さ）」を提案します。音楽理論の知識がなくても、誰でも直感的にプロのようなアドリブ演奏や作曲を楽しむことができます。

## 2. 解決する課題と提供する体験

**(動画冒頭で視聴者の共感を呼ぶパート）**

**従来の課題:**
音楽制作、特にアドリブ演奏には、複雑な音楽理論の知識と長年の訓練が必要でした。また、DAWを使ったマウスでの打ち込みは直感的ではなく、創造性を阻害することがあります。既存のAI作曲ツールは自動生成がメインで、「自分で演奏している感覚」を得るのは困難でした。

**Melody Flowが提供する体験:**

1.  **「私が弾いている」という感覚:**
    ユーザーが入力したリズム、音の長さ(Note On/Off)、強弱にAIが完璧に追従。自分の手で音楽を奏でているという確かな実感。
2.  **音楽理論からの解放:**
    AIがメロディを完全にサポートするため、ユーザーは理論を意識せず、リズム表現に没頭できます。
3.  **セッションの興奮と創造性の拡張:**
    AIとのリアルタイムな掛け合いから生まれる、予測不能でクリエイティブなフレーズとの出会い。

## 3. コアコンセプト：人間とAIの役割分担

**(プロダクトの核となる思想を説明するパート）**

Melody Flowの核は、音楽の要素を人間とAIで分担することです。

*   **ハーモニー（コード進行）**: システムが提供（自動伴奏）。
*   **メロディ（音程）**: **AIが担当。**
*   **リズム・ニュアンス（強弱・音の長さ）**: **人間が担当。**

ユーザーは「ドレミ」を考える必要はありません。キーボードを叩くだけで、そのリズムに最適なメロディが自動的に当てはめられます。

## 4. デモンストレーション（機能詳細）

**(実際の操作画面を見せながら機能を説明するパート）**

### 4.1. セットアップとAI生成

1.  **設定:** Web画面で「コード進行」と「音楽スタイル」（JAZZ風/POP風）を選択。BPM調整。
2.  **AI生成:** 「フレーズをAIに生成させる」ボタンを押すと、バックエンドのAI（LLM）が、コード進行全体に対して最適なメロディフレーズ群を瞬時に生成・準備します。

### 4.2. リアルタイム・セッション

1.  **演奏開始:** 自動伴奏がスタート。
2.  **リズム入力:** MIDIキーボードまたはPCのスペースキーでリズムを刻みます。
3.  **リアルタイム発音:** キーを叩いた瞬間、AIが準備していたメロディが1音ずつ、入力されたリズムと強さで発音されます。
    *   **楽器としての表現力:** Note On/Offに対応し、スタッカートやテヌートといったニュアンスを直感的に表現可能。（`main.js`）
4.  **自動追従:** 曲の進行に合わせて、AIが提案するメロディも自動的に次のコードのものへと切り替わります。

### 4.3. 可視化（ピアノロール）

*   ユーザーが演奏した内容（タイミング、音の長さ、音程）がリアルタイムで記録されます。
*   現在のコード進行の位置や、AIが提案する音のタイミングもガイド表示されます。

## 5. 技術的ハイライト

**(プロダクトの独自性と技術的優位性を解説するパート）**

Melody Flowの体験は、フロントエンドのリアルタイム技術と、バックエンドの高度なAI開発・制御技術の連携によって実現されています。

### 5.1. アーキテクチャとリアルタイム性の確保

*   **フロントエンド (JavaScript/Tone.js/WebMidi.js):** ユーザー入力を遅延なく処理し、高精度な音楽再生とスケジューリングを担当。（`main.js`）
*   **バックエンド (Python/FastAPI/PyTorch):** AIモデルをホストし、高速なAPI連携を担当。（`api.py`）
*   **リアルタイム性の担保（ハイブリッド方式）:**
    *   AI推論の遅延を避けるため、**演奏開始前にコード進行全体のフレーズを事前生成**する方式を採用。演奏中はフロントエンドがフレーズを選択・発音するだけなので、楽器と同等の応答速度を実現しました。

### 5.2. AIモデルの開発と生成制御（独自技術）

我々の最大の技術的特徴は、「効率的な学習によるカスタムモデル開発」と「ルールベースによる音楽理論的制御」を組み合わせ、高品質なメロディ生成を実現した点にあります。

#### 5.2.1. カスタムAIモデルの開発：効率的ファインチューニング

汎用的なAIモデルではなく、特定の音楽スタイルに特化したカスタムモデルを自前で開発しました。（`train_model.py`）

*   **ベースモデル:** MIDI生成特化LLM（`dx2102/llama-midi`）を採用。
*   **ファインチューニング (SFT):**
    *   **「Weimar Jazz Database (wjazzd)」**（高品質なジャズソロ採譜データセット）を用いて追加学習（Supervised Fine-Tuning）を実施。
    *   **効果:** AIはジャズ特有の「節回し」や「音楽的ボキャブラリー」を獲得し、より本格的なフレーズを生成可能になりました。
*   **効率的学習の技術的工夫:**
    *   LLMの学習には通常膨大な計算リソースが必要ですが、我々は以下の技術を駆使し、限られた環境での高速かつ効率的な学習を実現しました。
    *   **LoRA (Low-Rank Adaptation):** モデルの全パラメータを更新する代わりに、ごく一部のパラメータ（アダプター）のみを調整することで、メモリ使用量と学習時間を劇的に削減。
    *   **Unsloth:** 学習を高速化・省メモリ化する最新ライブラリを活用し、学習サイクルを高速化。
    *   **量子化 (4bit Quantization):** モデルのサイズを圧縮し、学習時および推論時のメモリ効率を向上。

#### 5.2.2. LogitsProcessorによる音楽理論的制御

ファインチューニングされたモデルは高い創造性を持ちますが、時としてコード進行に合わない音を生成してしまうことがあります。

*   **解決策：独自の「MelodyControlLogitsProcessor」の実装** (`melody_processor.py`)
    LLMが次の音を決定する直前のプロセス（Logits＝確率分布の計算）に介入する機構を開発。LLMの出力を音楽理論に基づいてリアルタイムで制約・誘導します。

**実装された制御ロジック:**

1.  **コード・スケール適合制御:**
    *   独自の音楽理論パーサー（`chord_name_parser.py`）で現在のコードを解析し、利用可能な音楽スケールを特定。
    *   スケールに含まれない音の出現確率を劇的に下げる（ペナルティを課す）処理を実行。
    *   **効果:** ハーモニーを崩さない、音楽的に正しいメロディ生成を実現。

2.  **メロディトレンド制御（自然な流れの誘導）:**
    *   直近で生成された数音の平均的な音の高さ（トレンドピッチ）をリアルタイムで計算。（`melody_processor.py`の`calc_trend`）
    *   次に生成される音が、そのトレンド周辺の音になるように確率を誘導。
    *   **効果:** 極端な音の跳躍を抑制し、滑らかで自然なメロディラインを形成。

**この「自前での効率的なファインチューニングによる創造性・専門性の獲得」と「LogitsProcessorによる理論的整合性の担保」の組み合わせが、Melody Flowの高品質なセッション体験の基盤となっています。**

## 6. 今後の展望

**(プロジェクトの将来性を示すパート）**

*   **AIモデルの高度化:**
    *   小節をまたいだ、より長く文脈のある自然なメロディライン（起承転結）の生成ロジックの導入。
    *   追加のデータセットによる対応スタイルの拡充（例: クラシック風、Lo-fi Hip Hopなど）。
*   **機能拡張:**
    *   演奏結果のMIDIエクスポート機能（DAWでの再編集）。
    *   ユーザーが自由にコード進行を入力できるカスタマイズ機能。

**まとめ:**
Melody Flowは、音楽制作のプロセスを根本から変える可能性を秘めた、新しい時代の「楽器」です。
