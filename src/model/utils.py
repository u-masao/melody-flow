import unsloth  # noqa: F401
import os
import sys

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LogitsProcessorList
from unsloth import FastLanguageModel

from src.model.melody_processor import NoteTokenizer

def _get_op_decorator():
    """ç’°å¢ƒå¤‰æ•°ã«å¿œã˜ã¦ã€weave.opã¾ãŸã¯ä½•ã‚‚ã—ãªã„ãƒ€ãƒŸãƒ¼ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã‚’è¿”ã™"""
    if os.getenv("APP_ENV", "production") != "production":
        try:
            import weave
            return weave.op
        except ImportError:
            print("âš ï¸  weave is not installed. Running without it.")

    # æœ¬ç•ªãƒ¢ãƒ¼ãƒ‰ã€ã¾ãŸã¯weaveãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ€ãƒŸãƒ¼ã‚’è¿”ã™
    def dummy_op(*args, **kwargs):
        def decorator(f):
            return f
        if args and callable(args[0]):
            return decorator(args[0])
        return decorator
    return dummy_op

# --- ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã‚’å®šç¾© ---
op = _get_op_decorator()
APP_ENV = os.getenv("APP_ENV", "production")


def load_model_and_tokenizer(model_path: str | None):
    """
    ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ‘ã‚¹ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã™ã€‚
    ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã®å ´åˆã¯Unslothã‚’ã€Hubã®ãƒ‘ã‚¹ã®å ´åˆã¯Transformersã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
    """
    if model_path is None:
        model_path = "models/llama-midi.pth/"
    print(f"ğŸ§  Loading model: {model_path}...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ”¥ Using device: {device}")

    try:
        model, tokenizer = None, None
        if os.path.isdir(model_path):
            print("-> Loading as local Unsloth model (4-bit)...")
            model, tokenizer = FastLanguageModel.from_pretrained(
                model_name=model_path, max_seq_length=4096, dtype=None, load_in_4bit=True
            )
        else:
            print(f"-> Loading as Hugging Face Hub model ({model_path})...")
            model = AutoModelForCausalLM.from_pretrained(
                model_path, torch_dtype=torch.bfloat16
            ).to(device)
            tokenizer = AutoTokenizer.from_pretrained(model_path)

        note_tokenizer_helper = NoteTokenizer(tokenizer)
        print("âœ… Model loaded successfully.")
        return model, tokenizer, note_tokenizer_helper, device
    except Exception as e:
        print(f"âŒ Fatal: Error loading model: {e}")
        return None, None, None, None


@op()
def generate_midi_from_model(
    model, tokenizer, device, prompt: str, processor, seed: int
) -> str:
    """
    ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨LogitsProcessorã‚’ä½¿ç”¨ã—ã¦MIDIãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    """
    torch.manual_seed(seed)
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    logits_processors = LogitsProcessorList([processor])
    output = model.generate(
        **inputs,
        max_new_tokens=128,
        temperature=0.75,
        pad_token_id=tokenizer.eos_token_id,
        logits_processor=logits_processors,
    )
    if APP_ENV != "production":
        # 'weave'ãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿å®Ÿè¡Œ
        try:
            import weave
            weave.summary({"allowed_notes": processor.note_tokenizer.ids_to_string(processor.allowed_token_ids)})
        except ImportError:
            pass # weaveãŒãªã‘ã‚Œã°ä½•ã‚‚ã—ãªã„
    return tokenizer.decode(output[0])


